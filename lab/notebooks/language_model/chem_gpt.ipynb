{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da46e484-baed-40f6-a09b-572b69132ce2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fine-tuning Chem-GPT for predicting the gap in PCQM4Mv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe8acf3-cd58-417c-a685-06b51f15a42c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/home/shayan/phoenix/graphite/')\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pandas\n",
    "from graphite.utilities.miscellaneous import count_parameters\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    rmse = mean_squared_error(labels, logits, squared=False)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "    smape = 1/len(labels) * np.sum(2 * np.abs(logits-labels) / (np.abs(labels) + np.abs(logits))*100)\n",
    "\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"smape\": smape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a61a631-6b2f-4a39-a6d6-8427671a54fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c479ed51-2f5b-4880-b4e3-1af89f0bc33e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at ncfrey/ChemGPT-19M and are newly initialized: ['transformer.h.19.attn.attention.bias', 'transformer.h.21.attn.attention.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.9.attn.attention.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.5.attn.attention.bias', 'score.weight', 'transformer.h.11.attn.attention.bias', 'transformer.h.17.attn.attention.bias', 'transformer.h.1.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.13.attn.attention.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ncfrey/ChemGPT-19M\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ncfrey/ChemGPT-19M\", num_labels=1, problem_type='regression').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be9d9cc-3bde-4b19-9a75-2036ea43dcce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19635968"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model) # 19M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5edadaab-0e7f-4342-a803-8ef626192546",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PCQM4Mv2SMILESDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, filepath='/data/pcqm4mv2_datahub/datasets/2d/pcqm4m-v2/raw/data.csv.gz'):\n",
    "        super().__init__()\n",
    "        idx = torch.load('/data/pcqm4mv2_datahub/datasets/2d/pcqm4m-v2/split_dict.pt')[split]\n",
    "        data = pandas.read_csv(filepath)\n",
    "        self.smiles, self.labels = data['smiles'][idx], data['homolumogap'][idx]\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"ncfrey/ChemGPT-19M\",  padding='max_length', truncation=True, max_length=142)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            model.pad_token = '[PAD]'\n",
    "            # model.resize_token_embeddings(len(tokenizer))\n",
    "            model.config.pad_token_id = 1 #tokenizer.vocab_size + 1 #model.config.eos_token_id\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "    def tokenize_function(self, item):\n",
    "        output = {k: v for k, v in self.tokenizer(item[\"text\"], padding=\"max_length\", truncation=True, max_length=142).items() if k in ['input_ids']}\n",
    "        output['labels'] = item['labels']\n",
    "        return output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = dict(text=self.smiles[idx])\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]]).float()\n",
    "        return self.tokenize_function(item)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb79bbb-e60a-488f-a364-f6b32f7f33b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PCQM4Mv2SMILESDataset('train')\n",
    "valid_dataset = PCQM4Mv2SMILESDataset('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd9816c-1e2e-491c-9dd9-ea6d1ff765fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specifiy the arguments for the trainer  \n",
    "training_args = TrainingArguments(\n",
    "    output_dir ='./bert_outputs',          \n",
    "    num_train_epochs = 30,     \n",
    "    per_device_train_batch_size = 64,   \n",
    "    per_device_eval_batch_size = 64,   \n",
    "    weight_decay = 0.01,               \n",
    "    learning_rate = 2e-5,\n",
    "    logging_dir = './bert_outputs/logs',            \n",
    "    save_total_limit = 10,\n",
    "    load_best_model_at_end = True,     \n",
    "    metric_for_best_model = 'rmse',    \n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\"\n",
    ") \n",
    "\n",
    "# Call the Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,                  \n",
    "    train_dataset = train_dataset,         \n",
    "    eval_dataset = valid_dataset,          \n",
    "    compute_metrics = compute_metrics_for_regression,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4cda65e-cc8a-4c1b-ba53-814983e24e64",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1689303"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e8577e3-d4ae-427d-bc40-df4b6c9b5c14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.args._n_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe68a0af-4e0d-405d-8ab2-bb42f79349a6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shayan/anaconda3/envs/gnn/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3378606\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1583730\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mshayanfazeli\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shayan/phoenix/graphite/lab/notebooks_l5/wandb/run-20221024_184022-2nn9fyyg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/shayanfazeli/huggingface/runs/2nn9fyyg\" target=\"_blank\">./bert_outputs</a></strong> to <a href=\"https://wandb.ai/shayanfazeli/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='272' max='1583730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    272/1583730 01:56 < 190:16:00, 2.31 it/s, Epoch 0.01/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bcd8b-1e55-4555-a616-5c0d7ef50d85",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Call the summary\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613e980-e6a7-4e40-9444-ff72aa77af80",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}